{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77ccd2f-053f-4b14-9ea7-8b0da9110144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elwin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:37: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.4.1)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Importing Required Libraries ---\n",
    "# This cell imports all the necessary libraries and modules for building a CNN-based Face Mask Detection system.\n",
    "# - TensorFlow/Keras: Deep learning framework used to build and train the CNN model.\n",
    "# - Layers (Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Input, GlobalAveragePooling2D): \n",
    "#   Building blocks for constructing the neural network architecture.\n",
    "# - Gradio (gr): Used to create an interactive web-based UI for real-time mask detection.\n",
    "# - Sequential & Model: Two ways to define Keras models (Sequential for linear stacking, Model for functional API).\n",
    "# - PIL (Image): Python Imaging Library for image manipulation and preprocessing.\n",
    "# - ImageDataGenerator: Utility for real-time data augmentation and loading images from directories.\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,Input,GlobalAveragePooling2D,MaxPooling2D,Conv2D,Flatten\n",
    "import gradio as gr\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# OUTPUT: A SciPy version warning may appear - this is a compatibility notice between NumPy and SciPy versions\n",
    "# and does NOT affect the functionality of the code. It can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2831ce0-276f-48a9-83a8-4e4b697dbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL: Importing NumPy ---\n",
    "# NumPy is imported for numerical operations such as array manipulation,\n",
    "# expanding dimensions of image arrays, and normalizing pixel values.\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b666255-cd3a-49a2-80e3-424e25341783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL: Defining the CNN Model Architecture ---\n",
    "# This cell builds a Sequential CNN model for binary classification (with mask / without mask).\n",
    "#\n",
    "# Architecture breakdown:\n",
    "# BLOCK 1: Two Conv2D layers (32 filters, 3x3 kernel, ReLU) -> BatchNormalization -> MaxPooling2D (2x2)\n",
    "#   - Extracts low-level features like edges and textures from 128x128x3 RGB input images.\n",
    "# BLOCK 2: Two Conv2D layers (64 filters) -> BatchNormalization -> MaxPooling2D\n",
    "#   - Captures mid-level features like shapes and patterns.\n",
    "# BLOCK 3: Two Conv2D layers (128 filters) -> BatchNormalization -> MaxPooling2D\n",
    "#   - Learns high-level features specific to mask/no-mask distinction.\n",
    "# CLASSIFIER:\n",
    "#   - Flatten: Converts 3D feature maps to 1D vector.\n",
    "#   - Dense(128, relu): Fully connected layer for learning complex combinations.\n",
    "#   - Dense(64, relu): Another FC layer for further abstraction.\n",
    "#   - Dropout(0.5): Randomly drops 50% of neurons during training to prevent overfitting.\n",
    "#   - Dense(1, sigmoid): Output layer with sigmoid activation for binary classification (0 or 1).\n",
    "\n",
    "cnn=Sequential([\n",
    "    Input(shape=(128,128,3)),\n",
    "\n",
    "    Conv2D(32,(3,3),activation='relu'),\n",
    "    Conv2D(32,(3,3),activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64,(3,3),activation='relu'),\n",
    "    Conv2D(64,(3,3),activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(128,(3,3),activation='relu'),\n",
    "    Conv2D(128,(3,3),activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4339a21-e2c2-4dae-8872-8618d97e1267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Setting Up Data Generators (Train, Validation, Test) ---\n",
    "# This cell uses ImageDataGenerator to load and augment the Face Mask dataset.\n",
    "#\n",
    "# TRAINING DATA AUGMENTATION (train_datagen):\n",
    "#   - rescale=1./255: Normalizes pixel values from [0,255] to [0,1] for faster convergence.\n",
    "#   - rotation_range=20: Randomly rotates images up to 20 degrees.\n",
    "#   - zoom_range=0.2: Randomly zooms in/out by 20%.\n",
    "#   - width/height_shift_range=0.1: Shifts images horizontally/vertically by 10%.\n",
    "#   - horizontal_flip=True: Randomly flips images horizontally.\n",
    "#   These augmentations help the model generalize better and reduce overfitting.\n",
    "#\n",
    "# VALIDATION & TEST DATA: Only rescaling (no augmentation) to evaluate on unmodified images.\n",
    "# All images are resized to 128x128, loaded in batches of 32, with binary class mode (2 classes).\n",
    "\n",
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "val_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "\n",
    "train_genrator=train_datagen.flow_from_directory(r\"C:\\Users\\elwin\\OneDrive\\Desktop\\CNN data\\Face Mask Dataset\\Train\",\n",
    "                                                 target_size=(128,128),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "val_genrator=val_datagen.flow_from_directory(r\"C:\\Users\\elwin\\OneDrive\\Desktop\\CNN data\\Face Mask Dataset\\Validation\",\n",
    "                                              target_size=(128,128),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='binary')\n",
    "test_genrator=val_genrator=test_datagen.flow_from_directory(r\"C:\\Users\\elwin\\OneDrive\\Desktop\\CNN data\\Face Mask Dataset\\Validation\",\n",
    "                                              target_size=(128,128),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='binary')\n",
    "\n",
    "# OUTPUT: The generator found:\n",
    "# - 10,000 training images belonging to 2 classes (WithMask, WithoutMask)\n",
    "# - 800 validation images belonging to 2 classes\n",
    "# - 800 test images belonging to 2 classes\n",
    "# This confirms the dataset is correctly loaded and split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ec6cf4-85fa-4138-a631-2c6d6ec30c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18432</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,424</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18432\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m2,359,424\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,655,649</span> (10.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,655,649\u001b[0m (10.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,655,201</span> (10.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,655,201\u001b[0m (10.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL: Compiling the Model and Displaying Architecture Summary ---\n",
    "# Compilation configures the model for training:\n",
    "#   - optimizer='adam': Adaptive learning rate optimizer, efficient for most deep learning tasks.\n",
    "#   - loss='binary_crossentropy': Loss function for binary classification (mask vs no mask).\n",
    "#   - metrics=['accuracy']: Tracks classification accuracy during training and evaluation.\n",
    "#\n",
    "# model.summary() prints the complete architecture with layer details.\n",
    "\n",
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "cnn.summary()\n",
    "\n",
    "# OUTPUT (model.summary):\n",
    "# The model has 3 convolutional blocks, each with 2 Conv2D layers, BatchNormalization, and MaxPooling2D.\n",
    "# Key observations from the summary:\n",
    "# - Input: 128x128x3 images -> progressively reduced spatial dimensions through pooling.\n",
    "# - Conv layers increase filters: 32 -> 64 -> 128 (capturing increasingly complex features).\n",
    "# - After Flatten: 18,432 features are fed to Dense layers.\n",
    "# - The Dense(128) layer has the most parameters (2,359,424) due to the large flattened input.\n",
    "# - Total params: 2,655,649 (10.13 MB) | Trainable: 2,655,201 | Non-trainable: 448 (from BatchNorm).\n",
    "# - Non-trainable params come from BatchNormalization's moving mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0b2aa5-9f84-41ee-ab70-163427e2385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - accuracy: 0.9029 - loss: 0.4211 - val_accuracy: 0.6787 - val_loss: 1.9083\n",
      "Epoch 2/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 1s/step - accuracy: 0.9446 - loss: 0.1744 - val_accuracy: 0.4963 - val_loss: 1.2252\n",
      "Epoch 3/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 907ms/step - accuracy: 0.9629 - loss: 0.1070 - val_accuracy: 0.9625 - val_loss: 0.1035\n",
      "Epoch 4/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 876ms/step - accuracy: 0.9673 - loss: 0.0900 - val_accuracy: 0.9737 - val_loss: 0.0850\n",
      "Epoch 5/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 878ms/step - accuracy: 0.9737 - loss: 0.0789 - val_accuracy: 0.9850 - val_loss: 0.0352\n",
      "Epoch 6/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 874ms/step - accuracy: 0.9746 - loss: 0.0775 - val_accuracy: 0.9775 - val_loss: 0.0736\n",
      "Epoch 7/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 857ms/step - accuracy: 0.9775 - loss: 0.0647 - val_accuracy: 0.9850 - val_loss: 0.0481\n",
      "Epoch 8/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 850ms/step - accuracy: 0.9790 - loss: 0.0598 - val_accuracy: 0.9337 - val_loss: 0.1740\n",
      "Epoch 9/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 862ms/step - accuracy: 0.9789 - loss: 0.0599 - val_accuracy: 0.7800 - val_loss: 0.5810\n",
      "Epoch 10/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 859ms/step - accuracy: 0.9804 - loss: 0.0579 - val_accuracy: 0.9750 - val_loss: 0.0819\n",
      "Epoch 11/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 852ms/step - accuracy: 0.9829 - loss: 0.0501 - val_accuracy: 0.9663 - val_loss: 0.0868\n",
      "Epoch 12/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 868ms/step - accuracy: 0.9802 - loss: 0.0573 - val_accuracy: 0.9150 - val_loss: 0.2468\n",
      "Epoch 13/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 855ms/step - accuracy: 0.9855 - loss: 0.0487 - val_accuracy: 0.9887 - val_loss: 0.0344\n",
      "Epoch 14/14\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 855ms/step - accuracy: 0.9835 - loss: 0.0486 - val_accuracy: 0.9962 - val_loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Training the CNN Model ---\n",
    "# This cell trains the model on the training data for 14 epochs with validation monitoring.\n",
    "#   - train_genrator: Feeds augmented training images in batches of 32.\n",
    "#   - epochs=14: The model will see the entire training dataset 14 times.\n",
    "#   - validation_data=val_genrator: Evaluates on validation set after each epoch to monitor overfitting.\n",
    "# The training history is stored in 'hist' for later analysis (e.g., plotting accuracy/loss curves).\n",
    "\n",
    "hist=cnn.fit(train_genrator,epochs=14,validation_data=val_genrator)\n",
    "\n",
    "# OUTPUT (Training Logs):\n",
    "# The model trained for 14 epochs (313 batches/epoch = 10,000 images / 32 batch size).\n",
    "# Key observations:\n",
    "# - Epoch 1:  Train Acc = 90.29%, Val Acc = 67.87% (model is learning, val is unstable initially)\n",
    "# - Epoch 5:  Train Acc = 97.37%, Val Acc = 98.50% (strong improvement, good generalization)\n",
    "# - Epoch 13: Train Acc = 98.55%, Val Acc = 98.87% (best val accuracy so far)\n",
    "# - Epoch 14: Train Acc = 98.35%, Val Acc = 99.62% (excellent final val accuracy)\n",
    "# - Val accuracy fluctuates at epochs 2,8,9,12 (signs of slight overfitting but recovers).\n",
    "# - Final result: ~98.35% training accuracy and ~99.62% validation accuracy.\n",
    "# - The model generalizes very well for face mask detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac00fc3-90ee-4e8d-8eba-fb1b03e28039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL: Setting Up the Gradio Web Interface for Face Mask Detection ---\n",
    "# This cell creates a Gradio-based interactive web app for real-time face mask prediction.\n",
    "#\n",
    "# Prediction function (mas_detection):\n",
    "#   1. Converts the uploaded numpy image to a PIL Image.\n",
    "#   2. Resizes to 128x128 (same as training input size).\n",
    "#   3. Normalizes pixel values to [0,1] by dividing by 255.\n",
    "#   4. Expands dimensions to add batch axis: (128,128,3) -> (1,128,128,3).\n",
    "#   5. Runs prediction through the trained CNN model.\n",
    "#   6. If prediction < 0.5 -> 'without mask'; else -> 'with mask' (sigmoid output interpretation).\n",
    "#\n",
    "# Gradio Interface:\n",
    "#   - Input: Image upload widget (numpy format).\n",
    "#   - Output: Textbox displaying prediction result with confidence score.\n",
    "\n",
    "models=cnn\n",
    "\n",
    "# image preprocessing funtion\n",
    "def mas_detection(image):\n",
    "    image=Image.fromarray(image)\n",
    "    image=image.resize((128,128))\n",
    "    image=np.array(image)/255.0\n",
    "    image=np.expand_dims(image,axis=0)\n",
    "\n",
    "    predictions=models.predict(image)[0][0]\n",
    "    if predictions < 0.5 :\n",
    "        return f'without mask (confidence :{predictions: 2f})'\n",
    "    else :\n",
    "        return f'with mask (confidence :{predictions: 2f})'\n",
    "\n",
    "interface=gr.Interface(\n",
    "    fn=mas_detection,\n",
    "    inputs=gr.Image(type='numpy',label='Upload Image'),\n",
    "    outputs=gr.Textbox(label='Prediction'),\n",
    "    title='mask image classifier',\n",
    "    description='upload an image to identify the with or without mask'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501974d0-1d9d-49dc-827b-2182ccc8f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Launching the Gradio Web Application ---\n",
    "# This launches the Gradio interface as a local web server.\n",
    "# Users can upload face images and get instant mask/no-mask predictions.\n",
    "\n",
    "interface.launch()\n",
    "\n",
    "# OUTPUT: The Gradio app is running on http://127.0.0.1:7861\n",
    "# An interactive iframe is embedded in the notebook for direct use.\n",
    "# When an image is uploaded, the model processes it (1/1 batch) and returns the prediction.\n",
    "# To create a publicly shareable link, use interface.launch(share=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838f2a9-b9ce-4886-b74a-89e63948569d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}